# GPT Character-Level Language Model

This is a simple Transformer-based character-level language model implemented in PyTorch. It uses self-attention, multi-head attention, and positional embeddings for text generation.

## Features
- Transformer architecture with self-attention
- Multi-head attention mechanism
- Positional embeddings for sequence modeling
- Training with AdamW optimizer and dropout regularization
